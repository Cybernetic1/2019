\input{../YKY-preamble.tex}

\title{Symmetric neural networks}
\author{YKY}

\begin{document}
\maketitle

\section{General case for $y = A x$}

\begin{equation}
\boxed{\mbox{original}} \quad y_j = \sum_i a_{ij} x_i
\end{equation}

Equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_j \cdot \sigma(x_j \; x_k) &=& y_k \quad \boxed{\mbox{original}} \nonumber\\
\sum_i \square + a_{kj} x_j + a_{jj} x_k &=& \sum_i \square + a_{jk} x_j + a_{kk} x_k
\end{eqnarray}
where $\square$ denotes ``the rest of the elements''.

This gives us a set of equations:
\begin{equation}
a_{kj} x_j + a_{jj} x_k = a_{jk} x_j + a_{kk} x_k \quad \quad \forall x_j, x_k
\end{equation}
for $(j \; k) \in \mathfrak{S}_n$.

By setting $x_j$ and $x_k$ to zero, this yields:
\begin{eqnarray}
a_{kj} &=& a_{jk} \quad \quad \forall j, k \nonumber \\
a_{jj} &=& a_{kk} \quad \quad \forall j, k .
\end{eqnarray}

In other words, the matrix $A$ is:
\begin{equation}
A = \alpha I + \beta 1 1^T
\end{equation}

\section{Case for $y_k = A_k x x$}

The general form of a ``quadratic'' vector function is:
\begin{equation}
y = (A x) \cdot x + B x + C
\end{equation}

We just focus on the quadratic term $(A x) \cdot x$:
\begin{equation}
\boxed{\mbox{original}} \quad y_k = \sum_j \left[ \sum_i a_{ij}^k x_i \right] x_j
\end{equation}

Equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_k \cdot \sigma(x_h \; x_k) &=& y_k \quad \boxed{\mbox{original}} \nonumber\\
\sum_{j \neq h,k} \sum_{i \neq h,k} \square + a_{kj} x_j + a_{jj} x_k &=& \sum_{j \neq h,k} \sum_{i \neq h,k} \square + a_{jk} x_j + a_{kk} x_k
\end{eqnarray}

\end{document}
