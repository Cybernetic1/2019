\input{../YKY-preamble.tex}

\title{Symmetric neural networks}
\author{YKY}

\begin{document}
\maketitle

\section{General case for $y = A x$}

\begin{equation}
\boxed{\mbox{original}} \quad y_j = \sum_i a_{ij} x_i .
\end{equation}

Equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_j ( \sigma(x_j \; x_k) x) &=& y_k \quad \boxed{\mbox{original}} \\
\sum_{i \neq j,k} a_{ij} x_i + a_{kj} x_j + a_{jj} x_k &=& \sum_{i \neq j,k} a_{ik} x_i + a_{jk} x_j + a_{kk} x_k . \nonumber
\end{eqnarray}

% for $(j \; k) \in \mathfrak{S}_n$.

Comparing coefficients yields:
\begin{eqnarray}
a_{ij} &=& a_{ik} \quad \quad \forall \; j, k, (i \neq j, k) \nonumber \\
a_{kj} &=& a_{jk} \quad \quad \forall \; j, k \nonumber \\
a_{jj} &=& a_{kk} \quad \quad \forall \; j, k .
\end{eqnarray}

In other words, the matrix $A$ is of the form:
\begin{equation}
A = \alpha I + \beta 1 1^T .
\end{equation}

\section{Case for $y_k = A_k x \cdot x$}

The general form of a ``quadratic'' vector function is:
\begin{equation}
y = (A x) \cdot x + B x + C .
\end{equation}

We just focus on the quadratic term $(A x) \cdot x$:
\begin{equation}
\boxed{\mbox{original}} \quad y_k = \sum_j \left[ \sum_i a_{ij}^k x_i \right] x_j .
\end{equation}
Note that the matrix $A$ is ``3D'' and has $N \times N \times N$ entries.

Equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_k ( \sigma(x_k \; x_h) x) &=& y_h \quad \boxed{\mbox{original}} \\
\sum_{j \neq h,k} \sum_{i \neq h,k} a_{ij}^k x_i x_j + a_{hh}^k x_k^2 + a_{kh}^k x_h x_k + a_{hk}^k x_k x_h + a_{kk}^k x_h^2 &=& \sum_{j \neq h,k} \sum_{i \neq h,k} a_{ij}^h x_i x_j + a_{hh}^h x_h^2 + a_{kh}^h x_k x_h + a_{hk}^h x_h x_k + a_{kk}^h x_k^2 \nonumber
\end{eqnarray}
which yields:
\begin{alignat}{3}
a_{ij}^h &= a_{ij}^k && \forall \; h,k, (i \neq h,k, j \neq h,k) \nonumber \\
a_{hh}^h &= a_{kk}^k && \forall \; h,k \nonumber \\
a_{hh}^k &= a_{kk}^h && \forall \; h,k \nonumber \\
a_{kh}^k + a_{hk}^k &= a_{kh}^h + a_{hk}^h \quad && \forall \; h,k .
\end{alignat}

\section{With output space ``folded in half''}

Now suppose the output is only $1/2$ the dimension of the input.  Define a new form of equivariance such that the input permutation would act on the output as ``folded in half''. 

In other words, equivariance is changed to:
\begin{equation}
\boxed{\mbox{swapped}} \quad y_k \cdot \sigma(x_k \; x_h) = y_h \mbox{  or  } y_{h-N/2} \quad \boxed{\mbox{original}} \end{equation}
where $\tau$ is $\sigma$ acting on $y$ as double its length and identifying $y_i = y_{i + N/2}$.

\subsection{Linear case}

Just notice that the dimension of $y$ is halved:
\begin{equation}
\boxed{\mbox{original}} \quad y_j = \sum_i a_{ij} x_i .
\end{equation}

``Folded'' equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_j ( \sigma(x_j \; x_k) x) %\mbox{ or } y_m ( \sigma(x_m \; x_k) x)
&=& y_k \quad \boxed{\mbox{original}} \\
\sum_{i \neq j,k} a_{ij} x_i + a_{kj} x_j + a_{jj} x_k &=& \sum_{i \neq j,k} a_{ik} x_i + a_{jk} x_j + a_{kk} x_k  \nonumber
% \mbox{or } \sum_{i \neq m,k} a_{im} x_i + a_{km} x_m + a_{mm} x_k & & \nonumber
\end{eqnarray}
% where $m = j - N/2$.  This gives rise to 2 sets of equations.
with the restriction $j \in \{ 1,..., N/2 \}$, and $k \in \{ 1,..., N \}$.

The constraints obtained are same as before, except that index ranges are different:
\begin{eqnarray}
a_{ij} &=& a_{ik} \quad \quad \forall \;  j, k, (i \neq j, k) \nonumber \\
a_{kj} &=& a_{jk} \quad \quad \forall \;  j, k \nonumber \\
a_{jj} &=& a_{kk} \quad \quad \forall \;  j, k \nonumber
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j > N/2, k \le N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j > N/2, k \le N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j > N/2, k \le N/2 \nonumber\\
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j \le N/2, k > N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j \le N/2, k > N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j \le N/2, k > N/2 \nonumber\\
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j > N/2, k > N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j > N/2, k > N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j > N/2, k > N/2
\end{eqnarray}

These constraints give rise to a matrix of this form (for the $6 \times 2$ case, numbers represent different colors):
\begin{equation}
\begin{tabular}{c c c c c c c}
5 & 1 & 1 & 2 & 3 & 4 & \\
1 & 5 & 1 & 2 & 3 & 4 & \\
1 & 1 & 5 & 2 & 3 & 4 & .
\end{tabular} 
\end{equation}This pattern is obtained from my Python code.

\subsection{Quadratic case}

\end{document}