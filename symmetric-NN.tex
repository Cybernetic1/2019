\input{../YKY-preamble.tex}

\usepackage{xeCJK}
\setCJKmainfont[BoldFont=SimHei,ItalicFont=KaiTi]{SimSun}
\usepackage{color}

\title{Symmetric neural networks}
\author{YKY}

\begin{document}
\maketitle

\section{General case for $y = A x$}

\begin{equation}
\boxed{\mbox{original}} \quad y_j = \sum_i a_{ij} x_i .
\end{equation}

Equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_j ( \sigma(x_j \; x_k) x) &=& \sigma \cdot y_j = y_k \quad \boxed{\mbox{original}} \\
\sum_{i \neq j,k} a_{ij} x_i + a_{kj} x_j + a_{jj} x_k &=& \sum_{i \neq j,k} a_{ik} x_i + a_{jk} x_j + a_{kk} x_k . \nonumber
\end{eqnarray}

% for $(j \; k) \in \mathfrak{S}_n$.

Comparing coefficients yields:
\begin{eqnarray}
a_{ij} &=& a_{ik} \quad \quad \forall \; j, k, (i \neq j, k) \nonumber \\
a_{kj} &=& a_{jk} \quad \quad \forall \; j, k \nonumber \\
a_{jj} &=& a_{kk} \quad \quad \forall \; j, k .
\end{eqnarray}

In other words, the matrix $A$ is of the form:
\begin{equation}
A = \alpha I + \beta 1 1^T .
\end{equation}

\section{Case for $y_k = A_k x \cdot x$}

The general form of a ``quadratic'' vector function is:
\begin{equation}
y = (A x) \cdot x + B x + C .
\end{equation}

We just focus on the quadratic term $(A x) \cdot x$:
\begin{equation}
\boxed{\mbox{original}} \quad y_k = \sum_j \left[ \sum_i a_{ij}^k x_i \right] x_j .
\end{equation}
Note that the matrix $A$ is ``3D'' and has $N \times N \times N$ entries.

Equivariance implies:
\begin{equation}
\boxed{\mbox{swapped}} \quad y_k ( \sigma(x_k \; x_h) \cdot x) = \sigma \cdot y_k = y_h \quad \boxed{\mbox{original}}
% \sum_{j \neq h,k} \sum_{i \neq h,k} a_{ij}^k x_i x_j + a_{hh}^k x_k^2 + a_{kh}^k x_h x_k + a_{hk}^k x_k x_h + a_{kk}^k x_h^2 &=& \sum_{j \neq h,k} \sum_{i \neq h,k} a_{ij}^h x_i x_j + a_{hh}^h x_h^2 + a_{kh}^h x_k x_h + a_{hk}^h x_h x_k + a_{kk}^h x_k^2 \nonumber
\end{equation}

\begin{eqnarray}
LHS &=& \sum_j \left[ \sum_{i \neq h,k} a^k_{ij} x_i + a^k_{hj} x_k + a^k_{kj} x_h \right] \sigma \cdot x_j \nonumber \\
&=& \sum_{j \neq h,k} \left[ \sum_{i \neq h,k} a^k_{ij} x_i + a^k_{hj} x_k + a^k_{kj} x_h \right] x_j
+ \left[ \sum_{i \neq h,k} a^k_{ih} x_i + a^k_{hh} x_k + a^k_{kh} x_h \right] x_k
+ \left[ \sum_{i \neq h,k} a^k_{ik} x_i + a^k_{hk} x_k + a^k_{kk} x_h \right] x_h \nonumber \\
&=& \sum_{j \neq h,k} \sum_{i \neq h,k} a^k_{ij} x_i x_j + \sum_{j \neq h,k} a^k_{hj} x_k x_j + \sum_{j \neq h,k} a^k_{kj} x_h x_j \nonumber \\
&& + \sum_{i \neq h,k} a^k_{ih} x_i x_k + a^k_{hh} x^2_k + a^k_{kh} x_h x_k \nonumber \\
&& + \sum_{i \neq h,k} a^k_{ik} x_i x_h + a^k_{hk} x_k x_h + a^k_{kk} x^2_h \nonumber \\
RHS &=& \sum_j \left[ \sum_i a_{ij}^h x_i \right] x_j \nonumber \\
&=& \sum_{j \neq h,k} \sum_{i \neq h,k} a^h_{ij} x_i x_j + \sum_{j \neq h,k} a^h_{kj} x_k x_j + \sum_{j \neq h,k} a^h_{hj} x_h x_j \nonumber \\
&& + \sum_{i \neq h,k} a^h_{ik} x_i x_k + a^h_{kk} x^2_k + a^h_{hk} x_h x_k \nonumber \\
&& + \sum_{i \neq h,k} a^h_{ih} x_i x_h + a^h_{kh} x_k x_h + a^h_{hh} x^2_h
\end{eqnarray}

Comparing coefficients yields:
\begin{alignat}{3}
a_{ij}^h &= a_{ij}^k && \forall \; h,k, (i \neq h,k, j \neq h,k) \nonumber \\
a_{kj}^h &= a_{hj}^k && \forall \; h,k, (j \neq h,k) \nonumber \\
a_{hj}^h &= a_{kj}^k && \forall \; h,k, (j \neq h,k) \nonumber \\
a_{ik}^h &= a_{ih}^k && \forall \; h,k, (i \neq h,k) \nonumber \\
a_{ih}^h &= a_{ik}^k && \forall \; h,k, (i \neq h,k) \nonumber \\
a_{kk}^h &= a_{hh}^k && \forall \; h,k \nonumber \\
a_{hh}^h &= a_{kk}^k && \forall \; h,k \nonumber \\
a_{hk}^h + a_{kh}^h &= a_{hk}^k + a_{kh}^k \quad && \forall \; h,k .
\end{alignat}

How many different colors?
\begin{equation}
\begin{tabular}{c c c c}
$N = 2$ .... & 6 & / 8 & = 75\% \\
$N = 3$ .... & 9 & / 27 & = 33.3\% \\
$N = 4$ .... & 11 & / 64 & = 17/2\% \\
$N = 5$ .... & 13 & / 125 & = 10.4\% \\
$N = 6$ .... & 15 & / 216 & = 6.9\%
\end{tabular}
\end{equation}

There would be $N$ \textbf{blocks} of $N \times N$ matrices.

All diagonals consists of 2 colors, regardless of $N$ (from 2nd and 3rd equations).  This leaves $N (N - 1)$ non-diagonal entries per block.

Non-diagonal entries of different blocks are equal, if the block indices are different from the row and column indices.  Out of $N$ blocks there would be 2 different sets of non-diagonal weights.  (This comes from the 1st equation.)

The last equation causes non-diagonal weights to have a certain symmetry about the diagonal.  

\section{With output space ``folded in half''}

Now suppose the output is only $1/2$ the dimension of the input.  Define a new form of equivariance such that the input permutation would act on the output as ``folded in half''. 

In other words, equivariance is changed to:
\begin{equation}
\boxed{\mbox{swapped}} \quad y_k \cdot \sigma(x_k \; x_h) = y_h \mbox{  or  } y_{h-N/2} \quad \boxed{\mbox{original}} \end{equation}
where $\tau$ is $\sigma$ acting on $y$ as double its length and identifying $y_i = y_{i + N/2}$.

\subsection{Linear case}

Just notice that the dimension of $y$ is halved:
\begin{equation}
\boxed{\mbox{original}} \quad y_j = \sum_i a_{ij} x_i .
\end{equation}

``Folded'' equivariance implies:
\begin{eqnarray}
\boxed{\mbox{swapped}} \quad y_j ( \sigma(x_j \; x_k) x) %\mbox{ or } y_m ( \sigma(x_m \; x_k) x)
&=& \sigma \cdot y_j = y_k \quad \boxed{\mbox{original}} \\
\sum_{i \neq j,k} a_{ij} x_i + a_{kj} x_j + a_{jj} x_k &=& \sum_{i \neq j,k} a_{ik} x_i + a_{jk} x_j + a_{kk} x_k  \nonumber
% \mbox{or } \sum_{i \neq m,k} a_{im} x_i + a_{km} x_m + a_{mm} x_k & & \nonumber
\end{eqnarray}
% where $m = j - N/2$.  This gives rise to 2 sets of equations.
with the restriction $j \in \{ 1,..., N/2 \}$, and $k \in \{ 1,..., N \}$.

The constraints obtained are same as before, except that index ranges are different:
\begin{eqnarray}
a_{ij} &=& a_{ik} \quad \quad \forall \;  j, k, (i \neq j, k) \nonumber \\
a_{kj} &=& a_{jk} \quad \quad \forall \;  j, k \nonumber \\
a_{jj} &=& a_{kk} \quad \quad \forall \;  j, k \nonumber
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j > N/2, k \le N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j > N/2, k \le N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j > N/2, k \le N/2 \nonumber\\
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j \le N/2, k > N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j \le N/2, k > N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j \le N/2, k > N/2 \nonumber\\
%\hline \nonumber\\
%a_{ij} &=& a_{ik} \quad \quad \forall \;  j > N/2, k > N/2, (i \neq j, k) \nonumber \\
%a_{kj} &=& a_{jk} \quad \quad \forall \;  j > N/2, k > N/2 \nonumber \\
%a_{jj} &=& a_{kk} \quad \quad \forall \;  j > N/2, k > N/2
\end{eqnarray}

These constraints give rise to a matrix of this form (for the $6 \times 3$ case, numbers represent different colors):
\begin{equation}
\begin{tabular}{c c c c c c c}
5 & 1 & 1 & 2 & 3 & 4 & \\
1 & 5 & 1 & 2 & 3 & 4 & \\
1 & 1 & 5 & 2 & 3 & 4 & .
\end{tabular} 
\end{equation}This pattern is obtained from my Python code.

NOTE:  The above pattern is verified to be NOT equivariant, there is a bug in the equivariant condition.

\subsection{Quadratic case}

\section{Training of the NN}

作者：zighouse

链接：\url{https://www.zhihu.com/question/327765164/answer/704606353}

来源：知乎

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

神经网络是对一类内部结构固定的非线性函数的俗称，这类函数是输出关于输入以及隐含内部状态的函数，输出与输入呈现非线性特性。当一份输出只与一份输入有关时，常用卷积神经网络来实现。当一份输出与一个相继表达的输入序列相关时，可以用回归神经网络来实现。一般地，神经网络可以技术性地分解成神经元的复合，这里的神经元是在这个神经网络中的一种最基本的非线性函数的俗称，管理着属于它的内部状态，并基于这些内部状态在神经网络中负责着分配到它的非线性处理。每多一重基本非线性函数的复合，则多一层神经元。如果在某一重复合中出现了两类或者更多类基本非线性函数项的合并，则出现了分支。

神经网络的权值是分解到具体神经元管理的一种内部状态。用反向传播方法来更新神经网络的权值是基于这样一个基本的假设：在一个确定的输入（或者输入序列）并产生当前输出的这个点（权值构成的线性空间中的点）上，输出在这个点上是连续的。即权值点的连续微小变化会导致输出点相应的连续微小变化。这样，当我们希望调节当前权值以使此输出向特定点靠拢时，就得出了基于权值空间中错误/误差/惩罚的梯度的反向传播算法。

如果想在某个神经网络中的两个权值间建立一种约束关系，这两个权值自然就不再相互独立，可以通过考查整个权值构成的线性空间，秩会变小。约束条件只要不改变连续假设，仍然可以求出带约束条件下的梯度。如果改变了连续假设，则意味着非线性分解不恰当，需要重新分解神经网络的基本结构。

Traditional neural network:
\begin{eqnarray}
\boxed{neuron} \quad & y = & \sigmoid \vect{w} \cdot \vect{x} \nonumber \\
\boxed{layer} \quad & y = & \sigmoid W \vect{x} \nonumber \\
\boxed{network} \quad & y = & \sigmoid W \circ \sigmoid W \; .... \; \vect{x} 
\end{eqnarray}

Quadratic neural network:
\begin{eqnarray}
\boxed{neuron} \quad & y = & \sigmoid W \vect{x} \cdot \vect{x} \nonumber \\
\boxed{layer} \quad & y = & \sigmoid W \vect{x} \cdot \vect{x} \nonumber \\
\boxed{network} \quad & y = & \sigmoid W \circ \sigmoid W \; .... \; \vect{x} 
\end{eqnarray}

Recall the classic back-prop algorithm:
\begin{eqnarray}
\frac{\partial E}{\partial W_{i j}} &=& o_i \delta_j \\
\delta_j &=& \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial \mathrm{net}_j} =
	\begin{cases}
	\displaystyle
	\frac{\partial L}{\partial \sigmoid(o_j)} \frac{\partial \sigmoid(o_j)}{\partial o_j} & \text{if $j$ = output neuron} \\
	\displaystyle
	\sum_l w_{j l} \delta_l \frac{\partial \sigmoid(o_j)}{\partial o_j} &  \text{if $j$ = inner neuron} 
	\end{cases}
\end{eqnarray}

Calculate $\frac{\partial U}{\partial W}$.  If they are linked then update together.


\end{document}